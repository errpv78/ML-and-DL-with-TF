{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why data preprocessing in ML and DL?\n",
    "1. Preventing early saturation of non-linear activation functions like sigmoid function.      2. Assuring that all input is in same range of values etc.\n",
    "\n",
    "### Vanishing Gradient Problem\n",
    "If we use a sigmoid function for our activation function, then, if z (the output of the neuron prior to the activate function) is very large or very small, the derivative will be approximately 0. In consequence, when we go to compute the gradient and update the weights, the change will be so infinitesimally small that the model wonâ€™t improve. The latter is known as the vanishing gradient problem.\n",
    "\n",
    "In normalizing the output of the neuron before it enters the activation function, we can ensure it remains close to 0 where the derivative highest.\n",
    "\n",
    "#### Standard deviation is equal to the square root of the variance.\n",
    "\n",
    "### Need for batch normalization\n",
    "The distribution of the activations in intermediate layers is constantly changing during training. This slows down the training process because each layer must learn to adapt themselves to a new distribution in every training step. This problem is known as internal covariate shift.\n",
    "\n",
    "### Batch Normalization\n",
    "Batch normalization is a method we can use to normalize the inputs of each layer, in order to fight the internal covariate shift problem.\n",
    "During training time, a batch normalization layer does the following:\n",
    "\n",
    "1. Calculate the mean and variance of the layers input.\n",
    "\n",
    "2. Normalize the layer inputs using the previously calculated batch statistics.\n",
    "\n",
    "3. Scale and shift in order to obtain the output of the layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten\n",
    "from keras.datasets import cifar10\n",
    "from keras.utils import normalize, to_categorical\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tfds.load('fashion_mnist', download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test': <PrefetchDataset shapes: {image: (28, 28, 1), label: ()}, types: {image: tf.uint8, label: tf.int64}>,\n",
       " 'train': <PrefetchDataset shapes: {image: (28, 28, 1), label: ()}, types: {image: tf.uint8, label: tf.int64}>}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
